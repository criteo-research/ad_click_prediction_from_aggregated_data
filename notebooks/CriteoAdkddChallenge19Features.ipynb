{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c666c6cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T18:01:34.235890Z",
     "start_time": "2022-02-09T18:01:33.103184Z"
    }
   },
   "source": [
    "# Training a model with all features on the Adkdd challenge dataset\n",
    "\n",
    "As explained in the research article, training directly on the aggregated data released in the challenge does not produce good results.\n",
    "The main reason is as follow:\n",
    "- the dataset contains several features with a large number (>50k) of modalities. \n",
    "- those features (in particular features 14 and 17) are both strongly predictive (removing one has a significant impact on the skyline) and very strongly correlated.\n",
    "- However, it is not reasonable to have one parameter for each pair of modality of those features (the model would just be too large to fit in memory.) I tried hashing the crossfeature, it does not work well because this important information about the correlation of those features is lost by hashing, and the resulting model significantly underperforms.\n",
    "- The best solution so far is to compute some target encodings of these features, to reduce their cardinality to a reasonable number ( < 1000 ) before aggregating.\n",
    "\n",
    "In this notebook:\n",
    "- we precompute some target encodings of features with many modalities on a set of held out granular data.\n",
    "- we read the full (granular) dataset released with  https://arxiv.org/pdf/2201.13123.pdf  (see also https://github.com/criteo-research/ad_click_prediction_from_aggregated_data ),  preprocess it to replace each feature by these target encodings, and aggregate the data\n",
    "- finally we train the RMF model on the resulting aggregated data\n",
    "\n",
    "Note the training is done with *fairly large pyspark session*. It was written and tested on Criteo infrastructure, *making it work from outside may require a few changes* to install pyspark and create a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4df54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from aggregated_models.myimports  import *\n",
    "import aggregated_models.myJupyterUtils as myJupyterUtils ## Remove stacktraces on Keyboardinterupt\n",
    "plt.style.use('ggplot')\n",
    "from aggregated_models.aggdataset import * \n",
    "import gzip\n",
    "from itertools import islice\n",
    "from aggregated_models.RawFeatureMapping import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8c0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67572997",
   "metadata": {},
   "source": [
    "## Downloading challenge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "306ecfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/challenge/\"\n",
    "if not os.path.exists(datapath):\n",
    "    print(f\"creating {datapath}\")\n",
    "    os.mkdir(datapath)\n",
    "    import urllib.request\n",
    "    # full granular train set (2.4G)\n",
    "    urllib.request.urlretrieve(\"http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset-raw-granular-data.csv.gz\",\n",
    "                               datapath + \"large_train.csv.gz\")\n",
    "    # challenge files\n",
    "    urllib.request.urlretrieve(\"http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset.zip\", \n",
    "                               datapath + \"challenge.zip\")\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(datapath + \"challenge.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(datapath)\n",
    "    # additional lines\n",
    "    urllib.request.urlretrieve(\"http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset-additional-test-data.csv.gz\",\n",
    "                               datapath + \"large_test.csv.gz\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b5106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.1G\r\n",
      "drwxr-xr-x 2 a.gilotte Domain Users  290 Jul  5 16:39 .\r\n",
      "drwxr-xr-x 5 a.gilotte Domain Users   99 Jul  5 15:59 ..\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users  26M Jul  5 16:13 X_test.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 2.9M Jul  5 16:13 X_train.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 241M Jul  5 16:13 aggregated_noisy_data_pairs.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users  15M Jul  5 16:13 aggregated_noisy_data_singles.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 285M Jul  5 16:13 challenge.zip\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 103M Jul  5 16:39 large_test.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 2.5G Jul  5 16:09 large_train.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 141K Jul  5 16:13 y_test.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users  16K Jul  5 16:13 y_train.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ../data/challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff6e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4cddb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T18:01:34.381003Z",
     "start_time": "2022-02-09T18:01:34.367909Z"
    }
   },
   "outputs": [],
   "source": [
    "datapath = \"../data/challenge/\"\n",
    "filename =  datapath + \"large_train.csv.gz\"\n",
    "filename_largetest =  datapath + \"large_test.csv.gz\"\n",
    "filename_smalltrain = datapath + \"small_train.csv.gz\"\n",
    "filename_smalltest =  datapath + \"data/test.csv.gz\"\n",
    "\n",
    "labels = [\"click\" , \"sale\"]\n",
    "allfeatures = ['hash_'+str(i) for i in range(0,19)]\n",
    "\n",
    "##  Using  \"large_test\" to compute target encodings\n",
    "## Aggregating large_train, training on aggregated data\n",
    "## Test metrics computed on test and/or small_train  (\"small_train\" is actually not used at all in training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d784889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a630c11",
   "metadata": {},
   "source": [
    "## Preparing encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "166ac5c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T18:15:02.663792Z",
     "start_time": "2022-02-03T18:14:53.917667Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename_largetest , dtype=np.int32 ,  nrows = 4_000_000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ede12927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T18:16:38.826989Z",
     "start_time": "2022-02-03T18:16:26.956946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash_0 9542 -> 666\n",
      "hash_3 762 -> 150\n",
      "hash_10 6417 -> 714\n",
      "hash_12 4312 -> 270\n",
      "hash_13 104 -> 82\n",
      "hash_14 441976 -> 542\n",
      "hash_16 1042 -> 258\n",
      "hash_17 94264 -> 665\n"
     ]
    }
   ],
   "source": [
    "logbase=2\n",
    "nbStd = 0.3\n",
    "gaussianStd=1\n",
    "mappings = {}\n",
    "sigma = 0\n",
    "\n",
    "for f in allfeatures:\n",
    "    mappings[f] = RawFeatureMapping.FromDF( f, df  )\n",
    "    size = mappings[f].Size\n",
    "    if size > 100:\n",
    "        df[\"d\"]=1\n",
    "        df_f = df[[f, \"click\", \"d\"]].groupby(f).sum().reset_index()\n",
    "        df_f[\"click\"] += np.random.normal( 0,sigma , len( df_f ))\n",
    "        df_f[\"d\"] += np.random.normal( 0,sigma , len( df_f ))  \n",
    "        df_f.loc[df_f['d'] <1, 'd'] = 1\n",
    "        df_f.loc[df_f['click'] <0, 'click'] = 0\n",
    "        mappings[f] = RawFeatureMapping.BuildCtrBucketsFromAggDf(f, df_f, logbase=logbase, nbStd=nbStd, gaussianStd=gaussianStd)\n",
    "        print(f, size, '->', mappings[f].Size ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b4753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T18:17:02.863499Z",
     "start_time": "2022-02-03T18:17:02.842466Z"
    }
   },
   "source": [
    "## Aggregating the large train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32443405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T18:17:17.862895Z",
     "start_time": "2022-02-03T18:17:17.841769Z"
    }
   },
   "outputs": [],
   "source": [
    "rawFeaturesSet = RawFeaturesSet(allfeatures, mappings )\n",
    "maxNbModalities= {f : 998 for f in allfeatures}\n",
    "maxNbModalities[\"default\"] = 1_000_000 \n",
    "cfset = CrossFeaturesSet(rawFeaturesSet , \"*&*\",maxNbModalities=maxNbModalities  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f014794c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-03T18:18:00.298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch 884        \r"
     ]
    }
   ],
   "source": [
    "    aggdata = AggDataset( cfset , [ \"display\" , \"click\" ])\n",
    "    df0 = pd.read_csv(  filename ,  nrows=1)\n",
    "    names = df0.columns\n",
    "    batch = 100_000\n",
    "    with  gzip. open(filename, \"rb\") as file:\n",
    "        header = file.readline()\n",
    "        i = 0\n",
    "        while True:\n",
    "            df = pd.read_csv(  file ,  nrows=batch , header=0 , names =names )\n",
    "            if len(df) < 1 :\n",
    "                break\n",
    "            i +=1\n",
    "            print( f\"processing batch {i}        \", end = '\\r' )\n",
    "            aggdata.aggregate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82a6ce0a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-03T18:18:01.820Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/aggdata_kdd', \"wb\") as handle:\n",
    "    aggdata.dump( handle )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598ed54",
   "metadata": {},
   "source": [
    "## reloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "963931a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T11:44:40.127928Z",
     "start_time": "2022-02-07T11:44:39.883117Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/aggdata_kdd', \"rb\") as handle:\n",
    "    aggdata = AggDataset.load( handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4b5d798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T11:44:46.575009Z",
     "start_time": "2022-02-07T11:44:45.861854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianMechanism epsilon:5 delta:1e-05 sigma:19.494904835891614\n"
     ]
    }
   ],
   "source": [
    "aggdata = aggdata.MakeDiffPrivate( 5 ,1e-5 , True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401020c",
   "metadata": {},
   "source": [
    "## Learning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca302e17",
   "metadata": {},
   "source": [
    "#### Note\n",
    "This part of the notebook was written to run on Criteo infrastructure.\n",
    "- *it requires access to fairly large a spark session.*\n",
    "- and it used Criteo internal library (thx) to create and configure this session.\n",
    "It should be possible to make it work with minor changes on another infrastructure with spark (replacing thx calls with your own calls to create the session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d98148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install thx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58434c1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T18:05:17.036322Z",
     "start_time": "2022-02-09T18:05:10.533277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from aggregated_models.myimports  import *\n",
    "import aggregated_models.myJupyterUtils as myJupyterUtils ## Remove stacktraces on Keyboardinterupt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from aggregated_models.agg_mrf_model import *\n",
    "from aggregated_models.validation import * \n",
    "from aggregated_models.aggLogistic import AggLogistic\n",
    "from aggregated_models.aggdataset import *\n",
    "from aggregated_models.experiment import *\n",
    "from aggregated_models.mrf_helpers  import *\n",
    "from thx.hadoop.spark_config_builder import create_remote_spark_session, SparkSession\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb7329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bcaa328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T18:05:17.057499Z",
     "start_time": "2022-02-09T18:05:17.037556Z"
    }
   },
   "outputs": [],
   "source": [
    "    def LLH(prediction, y):\n",
    "        llh = np.log(prediction) * y + np.log(1 - prediction) * (1 - y)\n",
    "        return sum(llh) / len(y)\n",
    "    def Entropy(y):\n",
    "        py = sum(y > 0) / len(y)\n",
    "        return Entropy_(py)\n",
    "    def Entropy_(py):\n",
    "        return py * np.log(py) + (1 - py) * np.log(1 - py)\n",
    "    def NLLH(prediction, y):\n",
    "        if any(prediction <= 0) or any(prediction >= 1):\n",
    "            return np.nan\n",
    "        h = Entropy(y)\n",
    "        llh = LLH(prediction, y)\n",
    "        return (h - llh) / h\n",
    "    allfeatures = ['hash_'+str(i) for i in range(0,19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1371929d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T18:05:18.734048Z",
     "start_time": "2022-02-09T18:05:17.058702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931843"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_smalltest =  \"../../challenge-release/data/test.csv.gz\"\n",
    "test = pd.read_csv(filename_smalltest, sep=',')\n",
    "test[\"clicks\"] =test[\"click\"]      \n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfd73afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thx.hadoop.spark_config_builder import create_remote_spark_session, SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a981b549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:59:35.509914Z",
     "start_time": "2022-02-08T11:58:52.360879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-07 14:49:58,587 - cluster_pack.filesystem - INFO - Resolved base filesystem: <class 'pyarrow.hdfs.HadoopFileSystem'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/home/a.gilotte/aggdata_public/.agg_model_venv/lib64/python3.6/site-packages/cluster_pack/filesystem.py:235: FutureWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  fs = EnhancedFileSystem(pyarrow.hdfs.connect(host=host, port=port))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-07 14:49:59,539 - cluster_pack.uploader - INFO - viewfs://root/user/a.gilotte/envs/.agg_model_venv.pex already exists\n",
      "2022-07-07 14:50:30,625 - cluster_pack.packaging - INFO - found editable requirements {'aggregated_models': '/mnt/nfs/home/a.gilotte/aggdata_public/aggregated_models'}\n",
      "2022-07-07 14:50:30,651 - thx.hadoop.spark_config_builder - INFO - applicationId: application_1657197191666_39117\n",
      "2022-07-07 14:50:30,653 - thx.hadoop.spark_config_builder - INFO - spark UI: http://10.188.159.37:31047\n"
     ]
    }
   ],
   "source": [
    "memory = '6g' # memory = '8g'\n",
    "ss = create_remote_spark_session('LearningFromAggData', 250, 8, memory=memory,\n",
    "                                 memoryOverhead='8g', driver_memory='32g',\n",
    "                                         properties=\n",
    "                                            [\n",
    "                                                ('spark.speculation', 'true'),\n",
    "                                                ('spark.speculation.interval', '4s'),\n",
    "                                                    ('spark.speculation.multiplier', '3'),\n",
    "                                                ('spark.speculation.quantile', '0.9'),                                                \n",
    "                                            ],\n",
    "                                 hadoop_file_systems=['viewfs://root', 'viewfs://prod-am6'])\n",
    "ss.sparkContext.setCheckpointDir(\"viewfs://prod-am6/tmp/a.gilotte/load/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba38d4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.188.159.37:31047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3-criteo-1607362448</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test Spark parallelize</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9a0cabe5f8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9168f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91fb623a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-05T09:36:20.135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "starting to train for 1000 iters. logging every 10\n",
      " auc=0.848,  nllh=0.249,  llh=-0.2434,  nbiters=10\n",
      " auc=0.856,  nllh=0.263,  llh=-0.2389,  nbiters=20\n",
      " auc=0.859,  nllh=0.272,  llh=-0.2360,  nbiters=30\n",
      " auc=0.861,  nllh=0.277,  llh=-0.2344,  nbiters=40\n",
      " auc=0.863,  nllh=0.280,  llh=-0.2336,  nbiters=50\n",
      " auc=0.863,  nllh=0.281,  llh=-0.2331,  nbiters=60\n",
      " auc=0.864,  nllh=0.282,  llh=-0.2327,  nbiters=70\n",
      " auc=0.864,  nllh=0.283,  llh=-0.2325,  nbiters=80\n",
      " auc=0.864,  nllh=0.284,  llh=-0.2323,  nbiters=90\n",
      " auc=0.865,  nllh=0.284,  llh=-0.2322,  nbiters=100\n",
      " auc=0.865,  nllh=0.284,  llh=-0.2321,  nbiters=110\n",
      " auc=0.865,  nllh=0.284,  llh=-0.2321,  nbiters=120\n",
      "simpleGradientStep iter=4     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "xp0 = Experiment( \"aggdata_kdd_mu5\" , ss )\n",
    "xp0.defineModel(\n",
    "        'data/aggdata_kdd',\n",
    "    f'''\n",
    "config_params = AggMRFModelParams( {allfeatures} ,  clicksCfs=\"*&*\", \n",
    "            nbSamples=1_000_000,\n",
    "            regulL2= 50,     \n",
    "            regulL2Click=1000,\n",
    "            sampleFromPY0 = True,  \n",
    "            maxNbRowsPerSlice = 250,\n",
    "            muStepSizeMultiplier = 5.0\n",
    "             )\n",
    "model = AggMRFModel ( aggdata,\n",
    "            config_params,\n",
    "            sparkSession= ss   )     \n",
    "''',\n",
    "        stepsize =0.01 )\n",
    "xp0.run( test , logevery = 10, nbiters = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99815667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdb5318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02d396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ff5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "aggmodels_crto_env",
   "language": "python",
   "name": "aggmodels_crto_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
