{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ed3497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:09:46.064050Z",
     "start_time": "2022-02-08T11:09:40.935471Z"
    }
   },
   "source": [
    "# Running on the criteo-adkkd competition\n",
    "\n",
    "In the criteo-adkdd competition (https://www.adkdd.org/2021-privacy-ml-competition) the goal was to learn a click model from aggregated data.\n",
    "However, some small granular datasets were also available. This notebook train a model using only the aggregated data released during the competition.\n",
    "\n",
    "- it downloads the datasets\n",
    "- it reads the noisy aggregated files and build an \"aggdata\" structure adapted to the training of RMF. In this structure, all features and crossfearures are \"hashed\" (actually just a modulo) to 100K modalities.\n",
    "- it trains a RMF model, using only a subset of 11 features (out of 18).\n",
    "\n",
    "The main reason why we use a subset of the features is that the method does not work well with all features. (Scalability issues are another non trivial reason, but can be solved by using the pyspark training). \n",
    "The main problem is that some of the features are very strongly correlated, and the information on those correlations is lost when hashing. A workaround would be to first project these features (eg with target encodings) two a smaller number a modalities (say 1000), so that the full crossfeatures, without hashing can be modelled. However this does not work well with the preaggregated data available during the competition, for several reasons:\n",
    "- the data on unfrequent modalities pair was filtered out, making it impossible to reconsruct fully the crossfeatures (on a few pairs of features, it is a significative part of the samples which are misisng)\n",
    "- the noise becomes no longer negligible when re-aggregating (we sum together several instances of the noise, leading to a higher noise variance than if we directly aggregate on the target encoded data.\n",
    "- finaly, the rpe-aggregated data do not allow to build some good target encodings (or we would learn the model on the same set where those target encodings are trained; this typically leads to strong ovefitting)\n",
    "\n",
    "A model with all features is trained in the other notebook, which directlty aggregates on target encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1d9e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-08 13:26:24,364 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\n",
      "failed to load pyspark\n",
      "failed to load pyspark\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from aggregated_models.myimports  import *\n",
    "import aggregated_models.myJupyterUtils as myJupyterUtils ## Remove stacktraces on Keyboardinterupt\n",
    "plt.style.use('ggplot')\n",
    "from aggregated_models.aggdataset import * \n",
    "import gzip\n",
    "from itertools import islice\n",
    "from aggregated_models.RawFeatureMapping import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dbb9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/challenge/\"\n",
    "# filename =  datapath + \"large_train.csv.gz\"\n",
    "# filename_largetest =  datapath + \"large_test.csv.gz\"\n",
    "\n",
    "singleAggFile = datapath + \"aggregated_noisy_data_singles.csv.gz\"\n",
    "pairsAggFile  = datapath + \"aggregated_noisy_data_pairs.csv.gz\"\n",
    "\n",
    "labels = [\"click\" , \"sale\"]\n",
    "allfeatures = ['hash_'+str(i) for i in range(0,19)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7503a084",
   "metadata": {},
   "source": [
    "## Downloading challenge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4530ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/challenge/\"\n",
    "if not os.path.exists(datapath):\n",
    "    print(f\"creating {datapath}\")\n",
    "    os.mkdir(datapath)\n",
    "    import urllib.request\n",
    "    # full granular train set (2.4G)\n",
    "    urllib.request.urlretrieve(\"http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset-raw-granular-data.csv.gz\",\n",
    "                               datapath + \"large_train.csv.gz\")\n",
    "    # challenge files\n",
    "    urllib.request.urlretrieve(\"http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset.zip\", \n",
    "                               datapath + \"challenge.zip\")\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(datapath + \"challenge.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(datapath)\n",
    "    # additional lines\n",
    "    urllib.request.urlretrieve(\"http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset-additional-test-data.csv.gz\",\n",
    "                               datapath + \"large_test.csv.gz\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9ef0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.1G\r\n",
      "drwxr-xr-x 2 a.gilotte Domain Users  290 Jul  5 16:39 .\r\n",
      "drwxr-xr-x 5 a.gilotte Domain Users  150 Jul  7 16:46 ..\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users  26M Jul  5 16:13 X_test.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 2.9M Jul  5 16:13 X_train.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 241M Jul  5 16:13 aggregated_noisy_data_pairs.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users  15M Jul  5 16:13 aggregated_noisy_data_singles.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 285M Jul  5 16:13 challenge.zip\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 103M Jul  5 16:39 large_test.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 2.5G Jul  5 16:09 large_train.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users 141K Jul  5 16:13 y_test.csv.gz\r\n",
      "-rw-r--r-- 1 a.gilotte Domain Users  16K Jul  5 16:13 y_train.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ../data/challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba6c63",
   "metadata": {},
   "source": [
    "# Preparing Aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b25387f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:10:15.572807Z",
     "start_time": "2022-02-08T11:10:14.915816Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(singleAggFile  ) \n",
    "df[\"d\"] = df[\"count\"] * (df[\"count\"]>0) +1\n",
    "df[\"click\"] = df[\"nb_clicks\"] * (df[\"nb_clicks\"] > 0)\n",
    "df[\"sale\"] = df[\"nb_sales\"] * (df[\"nb_sales\"] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a02d6cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:10:16.739885Z",
     "start_time": "2022-02-08T11:10:15.735698Z"
    }
   },
   "outputs": [],
   "source": [
    "mappings = {}\n",
    "for i in sorted(set( df.feature_1_id.values  )):\n",
    "    f = f\"hash_{i}\"\n",
    "    df_f = df[ df.feature_1_id == i \n",
    "             ].rename({ \"feature_1_value\":f }, axis=1)\n",
    "    size = len(df_f)\n",
    "    mappings[f] = RawFeatureMapping.FromDF(f, df_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a816c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:10:34.678534Z",
     "start_time": "2022-02-08T11:10:34.621572Z"
    }
   },
   "outputs": [],
   "source": [
    "## Hashing all features and all crossfeatures to 100K modalities\n",
    "rawFeaturesSet = RawFeaturesSet(allfeatures, mappings )\n",
    "maxNbModalities= {f : 100_000 for f in allfeatures}\n",
    "maxNbModalities[\"default\"] = 1_000_000 \n",
    "cfset = CrossFeaturesSet(rawFeaturesSet , \"*&*\",maxNbModalities=maxNbModalities )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c05af44d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:13:43.736837Z",
     "start_time": "2022-02-08T11:13:41.349240Z"
    }
   },
   "outputs": [],
   "source": [
    "aggdata = AggDataset( cfset , [ \"display\" , \"click\", \"sale\", \"variance\" ])\n",
    "aggdata = AggDataset( cfset , [ \"display\" , \"click\" ])\n",
    "df = pd.read_csv(singleAggFile ) \n",
    "for f in allfeatures:\n",
    "    i = int( f.split('_')[1] )\n",
    "    df_f = df[ df.feature_1_id == i \n",
    "             ].rename({ \"feature_1_value\":f }, axis=1)\n",
    "    df_f = mappings[f].Map(  df_f )\n",
    "    df_f[\"variance\"] = 17*17\n",
    "    \n",
    "    df_f =df_f.groupby(f).sum().reset_index()\n",
    "    d = cfset.encodings[f].ProjectPandasDF(df_f , \"count\") \n",
    "    c = cfset.encodings[f].ProjectPandasDF(df_f , \"nb_clicks\")  \n",
    "    s = cfset.encodings[f].ProjectPandasDF(df_f , \"nb_sales\")  \n",
    "    variance = cfset.encodings[f].ProjectPandasDF(df_f , \"variance\")  \n",
    "    aggdata.aggDisplays[ f ].Data += d\n",
    "    aggdata.aggClicks[ f ].Data += c\n",
    "    # aggdata.aggregations[\"sale\"][f].Data += s\n",
    "    # aggdata.aggregations[\"variance\"][f].Data += variance    \n",
    "\n",
    "for k in aggdata.aggregations:    \n",
    "    aggdata.AggregationSums[k] = np.median( [ aggdata.aggregations[k][ f ].Data.sum()  for f in allfeatures]  )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c7da770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:13:55.116846Z",
     "start_time": "2022-02-08T11:13:45.715510Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(pairsAggFile ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e44551c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:14:40.382533Z",
     "start_time": "2022-02-08T11:13:57.819640Z"
    }
   },
   "outputs": [],
   "source": [
    "for cf in aggdata.aggDisplays:\n",
    "    if cf in allfeatures:\n",
    "        continue\n",
    "    f = cf.split(\"&\")[0]\n",
    "    f2 = cf.split(\"&\")[1]    \n",
    "    i = int(f.split(\"_\")[1])       \n",
    "    i2 = int(f2.split(\"_\")[1])       \n",
    "    \n",
    "    df_f = pd.concat(  [ df[ (df.feature_1_id == i) &(df.feature_2_id == i2) \n",
    "             ].rename({ \"feature_1_value\":f, \"feature_2_value\":f2 }, axis=1),\n",
    "                        df[ (df.feature_1_id == i2) &(df.feature_2_id == i) \n",
    "             ].rename({ \"feature_1_value\":f2, \"feature_2_value\":f }, axis=1),\n",
    "                       ])          \n",
    "    df_f = mappings[f].Map(  df_f )\n",
    "    df_f = mappings[f2].Map(  df_f )\n",
    "    df_f[\"variance\"] = 17*17\n",
    "    \n",
    "    df_f =df_f.groupby([f,f2]).sum().reset_index()\n",
    "    \n",
    "    d = cfset.encodings[cf].ProjectPandasDF(df_f , \"count\") \n",
    "    c = cfset.encodings[cf].ProjectPandasDF(df_f , \"nb_clicks\")  \n",
    "    s = cfset.encodings[cf].ProjectPandasDF(df_f , \"nb_sales\")  \n",
    "    variance = cfset.encodings[cf].ProjectPandasDF(df_f , \"variance\")      \n",
    "    \n",
    "    aggdata.aggDisplays[ cf ].Data += d\n",
    "    aggdata.aggClicks[ cf ].Data += c    \n",
    "#    aggdata.aggregations[\"sale\"][cf].Data += s    \n",
    "#    aggdata.aggregations[\"variance\"][cf].Data += variance        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7130fd22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:15:14.108320Z",
     "start_time": "2022-02-08T11:15:14.017159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50992603"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( [ len( aggdata.aggregations[\"click\"][cf].Data ) for cf in aggdata.aggregations[\"click\"] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969bca53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T11:15:25.295900Z",
     "start_time": "2022-02-08T11:15:22.818282Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/aggdata_officialcompetition_hash100k', \"wb\") as handle:\n",
    "    aggdata.dump( handle )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a589c1",
   "metadata": {},
   "source": [
    "## Reloading aggdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634a3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/aggdata_officialcompetition_hash100k', \"rb\") as handle:\n",
    "    aggdata = AggDataset.load( handle )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e58d52",
   "metadata": {},
   "source": [
    "# Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3cf738a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T19:57:59.579212Z",
     "start_time": "2022-02-07T19:57:58.009206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:Failed to load pyspark. Maybe it is not installed in your environement? This is Ok if you plan to use only in-memory training.\n"
     ]
    }
   ],
   "source": [
    "from aggregated_models.agg_mrf_model import *\n",
    "from aggregated_models.validation import * \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99390b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation = MetricsComputer(\"click\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26794573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbf27c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf = pd.read_csv( datapath + \"X_train.csv.gz\" , sep=',')\n",
    "smalldf[\"clicks\"]  = pd.read_csv( datapath + \"y_train.csv.gz\" , sep=',')[\"click\"]\n",
    "smalldf[\"click\"] = smalldf[\"clicks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54e131c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T19:58:02.098531Z",
     "start_time": "2022-02-07T19:57:59.621841Z"
    }
   },
   "outputs": [],
   "source": [
    "features11 = [allfeatures[i] for i in[0, 1, 2, 3, 4, 6, 8, 10, 13, 15, 16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d9edd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = AggMRFModelParams( features11 ,  clicksCfs=\"*&*\", \n",
    "            nbSamples=100_000,\n",
    "            regulL2= 50,     \n",
    "            regulL2Click=1000,\n",
    "            sampleFromPY0 = True,  \n",
    "            maxNbRowsPerSlice = 250,\n",
    "            muStepSizeMultiplier = 5.0\n",
    "             )\n",
    "model = AggMRFModel ( aggdata, config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60791426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLLH=0.2304, NMSE=0.1949       \n",
      "NLLH=0.2539, NMSE=0.2174       \n",
      "simpleGradientStep iter=11     \r"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    model.fit(20)\n",
    "    print( Validation.run(model,smalldf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff00587",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv( datapath + \"X_test.csv.gz\" , sep=',')\n",
    "test[\"clicks\"]  = pd.read_csv( datapath + \"y_test.csv.gz\" , sep=',')[\"click\"]\n",
    "test[\"click\"] = test[\"clicks\"]\n",
    "\n",
    "print( Validation.run(model,test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c89982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "aggregated_models_env",
   "language": "python",
   "name": "aggregated_models_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
